demonstrate that training noise produces D4-driven weight drift.
**  consult ...07_July/tensorflow-wrapper/  **
**  see ../comparing-zone-points/commentary_figures.txt @ training noise **

1. Pick the noise source
   Refresher: drift can come from 3-4 sources: [1] data noise, [2] algorithmic 
   discretization, [3] broken symmetry, and [4] interal interactions/learned structures.
   Types [1] and [2] are easy to test with basic cases; [3] should be straight-
   -forward enough to plot out; most cohesive tests of [1] can come from
   MNIST/etc, 
- [1] y contradictions: (v_i,y), (v_i,-y) for each v_i in range of interest (e.g. 
   np.linspace(-4,4,800)).  Example of type (1) noise.
- [1] v_i noise: (v_i+epsilon, y), (v_i-epsilon, y)
- [1] x noise: k >= 2 of {(x_k,y)}
- MNIST most overlapping but alternative samples, computed
  by calculating cosine dist of each 7 x each 9.
- [2] MNIST batchwise: ( collection of 7s ) before ( collection of 9s ); 
  batches over balanced mixtures of 7s, 9s; small batches over random 
  mixtures of 7s, 9x.
- something involving stochastic gradient descent?

2. Pick the analysis
- Serial: For each of the above noise sources, apply the backprop gradient
  from each of the data samples individually (or the different collections
  in sequence) in different batches. See where each weight or v_i went 
  based on the two 'symmetric' data batches.
- Parallel: Don't apply backprop. Simply calculate the difference between the 
  would-be changes if one batch had been trained on vs. a different one.


  



  -----

 New D4: a 2nd recursive backprop on D2. ie, if d is a differential operator,
 D4 = D2 d D2 = (sds)d(sds) ie s(x)d(s(x))d(s(x)d(s(x)))
 The most noticeable qualitative change is that now gamma and beta coincide.  
 The curves still look qualitatively the same and have the same loose structure
 D4's numbers make more sense (ie, 0 signal when s(x)=y* <=> 0 noise due to 
 signal). Still have same # of equilibrium points on either side of the 
 target (unless |target y| >= 1) which seem clearly attractive not repulsive 
 from the empirical plots. 

 True enough, it might suggest a new kind of calculus for analyzing backprop:
 weights-mod-x receive error signal based on loss function (least squares 
 easy case) being passed back and based on the activations that were passed  
 forward subject to the chain rule. If signal follows that function, the 
 end result is well-behaved. But if the weights also/alternatively change 
 due to drift, which is itself a function of whether the error signal
 is increasing or decreasing in magnitude swept across possible values of
 the weight. But now that beta=gamma, weights that go to this region are
 then ready respond most strongly to future signals. ie, with high SNR,
 they proceed to the target value; but if there's noise, they proceed
 instead to 

 Interestingly, if sigm=tanh and y->1 (or beyond, degenerately), then 
 the one remaining attractor goes to artanh(1/3) = .3466.
 For p=2, the attractor when y=1, 0.2582, is the value x making
 1/4 == x/sqrt(1+x**2). aka inverse(sigm_{p=2})(0.25)
 erf: 0.3578; 
