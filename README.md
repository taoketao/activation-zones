This repo has several scripts and plots involving my 2022 zones project.

The project: I observed that a neural network's nonlinear activation 
functions such as tanh or logistic saturate in surprising ways. In 
networks using these nonlinearities, weights learning via backpropagation 
experience different dynamics based on their numerical value - a phenomenon
not present in linear or RELU networks.  [ todo: finish writing this once
main text is written. ]

The code in this repo catalogs scripts and notebooks used in experiments 
that test the theoretical results against empirical neural networks.

Programs: 
...
